#!/usr/bin/env python3
"""
Exemplo de uso da integra√ß√£o LLM
Melhoria #6 - Integra√ß√£o com Modelos de Linguagem
"""

import asyncio
import logging
from llm_manager import LLMManager, LLMRequest, LLMModel

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def example_basic_usage():
    """Exemplo b√°sico de uso"""
    print("üöÄ Exemplo b√°sico de uso da integra√ß√£o LLM")
    
    # Inicializar gerenciador
    llm_manager = LLMManager()
    
    # Criar requisi√ß√£o
    request = LLMRequest(
        prompt="Como implementar um sistema de cache distribu√≠do?",
        agent_id="carlos_eduardo_santos",
        context="Estamos desenvolvendo uma aplica√ß√£o web de alta escala",
        temperature=0.3,
        max_tokens=1024
    )
    
    # Gerar resposta
    response = await llm_manager.generate_response(request)
    
    print(f"‚úÖ Resposta gerada:")
    print(f"   Modelo: {response.model_used}")
    print(f"   Provedor: {response.provider}")
    print(f"   Tokens: {response.tokens_used}")
    print(f"   Custo: ${response.cost:.4f}")
    print(f"   Tempo: {response.response_time:.2f}s")
    print(f"   Cache: {'Sim' if response.cached else 'N√£o'}")
    print(f"\nüìù Conte√∫do:\n{response.content[:200]}...")


async def example_multiple_agents():
    """Exemplo com m√∫ltiplos agentes"""
    print("\nü§ù Exemplo com m√∫ltiplos agentes")
    
    llm_manager = LLMManager()
    
    # Requisi√ß√µes para diferentes agentes
    requests = [
        LLMRequest(
            prompt="Qual a melhor arquitetura para um e-commerce?",
            agent_id="carlos_eduardo_santos",
            priority="high"
        ),
        LLMRequest(
            prompt="Como melhorar a UX do checkout?",
            agent_id="isabella_santos",
            priority="high"
        ),
        LLMRequest(
            prompt="Estrat√©gia de testes para e-commerce",
            agent_id="lucas_pereira",
            priority="normal"
        )
    ]
    
    # Processar em paralelo
    tasks = [llm_manager.generate_response(req) for req in requests]
    responses = await asyncio.gather(*tasks)
    
    for i, response in enumerate(responses):
        agent_id = requests[i].agent_id
        print(f"\nüë§ Resposta de {agent_id}:")
        print(f"   Modelo: {response.model_used}")
        print(f"   Custo: ${response.cost:.4f}")
        print(f"   Resumo: {response.content[:100]}...")


async def example_with_fallback():
    """Exemplo demonstrando sistema de fallback"""
    print("\nüîÑ Exemplo com sistema de fallback")
    
    llm_manager = LLMManager()
    
    # Requisi√ß√£o com modelo espec√≠fico que pode falhar
    request = LLMRequest(
        prompt="Explique machine learning em termos simples",
        agent_id="ana_beatriz_costa",
        model_preference=LLMModel.GPT_4,  # Pode n√£o estar dispon√≠vel
        temperature=0.7
    )
    
    response = await llm_manager.generate_response(request)
    
    print(f"‚úÖ Resposta com fallback:")
    print(f"   Modelo solicitado: {request.model_preference.value}")
    print(f"   Modelo usado: {response.model_used}")
    print(f"   Funcionou fallback: {'Sim' if response.model_used != request.model_preference.value else 'N√£o'}")


async def example_cache_demonstration():
    """Exemplo demonstrando cache"""
    print("\nüíæ Exemplo demonstrando cache")
    
    llm_manager = LLMManager()
    
    request = LLMRequest(
        prompt="O que √© DevOps?",
        agent_id="mariana_rodrigues",
        use_cache=True
    )
    
    # Primeira requisi√ß√£o
    print("üì§ Primeira requisi√ß√£o (sem cache)...")
    response1 = await llm_manager.generate_response(request)
    print(f"   Tempo: {response1.response_time:.2f}s")
    print(f"   Cache: {'Sim' if response1.cached else 'N√£o'}")
    
    # Segunda requisi√ß√£o (deve usar cache)
    print("üì§ Segunda requisi√ß√£o (com cache)...")
    response2 = await llm_manager.generate_response(request)
    print(f"   Tempo: {response2.response_time:.2f}s")
    print(f"   Cache: {'Sim' if response2.cached else 'N√£o'}")
    
    # Verificar se o conte√∫do √© o mesmo
    print(f"   Conte√∫do id√™ntico: {'Sim' if response1.content == response2.content else 'N√£o'}")


async def example_cost_monitoring():
    """Exemplo de monitoramento de custos"""
    print("\nüí∞ Exemplo de monitoramento de custos")
    
    llm_manager = LLMManager()
    
    # Fazer algumas requisi√ß√µes
    requests = [
        LLMRequest(prompt=f"Pergunta {i+1}: Como otimizar performance?", agent_id="sofia_oliveira")
        for i in range(3)
    ]
    
    total_cost = 0
    for i, request in enumerate(requests):
        response = await llm_manager.generate_response(request)
        total_cost += response.cost
        print(f"   Requisi√ß√£o {i+1}: ${response.cost:.4f}")
    
    print(f"üíµ Custo total: ${total_cost:.4f}")
    
    # Obter estat√≠sticas
    stats = llm_manager.get_stats()
    print(f"üìä Estat√≠sticas:")
    print(f"   Total de requisi√ß√µes: {stats['total_requests']}")
    print(f"   Taxa de sucesso: {stats['success_rate']:.1f}%")
    print(f"   Taxa de cache: {stats['cache_hit_rate']:.1f}%")


async def example_health_check():
    """Exemplo de health check"""
    print("\nüè• Exemplo de health check")
    
    llm_manager = LLMManager()
    
    health = await llm_manager.health_check()
    
    print(f"üîç Status geral: {health['status']}")
    print(f"üìà Estat√≠sticas: {health['statistics']}")
    
    print("\nüîå Status dos provedores:")
    for provider, status in health['providers'].items():
        emoji = "‚úÖ" if status['status'] == 'healthy' else "‚ùå"
        print(f"   {emoji} {provider}: {status['status']}")
        if 'error' in status:
            print(f"      Erro: {status['error']}")


async def example_model_selection():
    """Exemplo de sele√ß√£o de modelos"""
    print("\nüéØ Exemplo de sele√ß√£o de modelos")
    
    llm_manager = LLMManager()
    
    # Obter modelos dispon√≠veis
    models = await llm_manager.get_available_models()
    
    print("üìã Modelos dispon√≠veis:")
    for model in models[:5]:  # Mostrar apenas os primeiros 5
        print(f"   ‚Ä¢ {model['model']} ({model['provider']})")
        print(f"     Custo: ${model['cost_per_token']:.6f}/token")
        print(f"     Max tokens: {model['max_tokens']}")


async def main():
    """Fun√ß√£o principal com todos os exemplos"""
    print("üéØ CWB Hub - Exemplos de Integra√ß√£o LLM")
    print("=" * 50)
    
    try:
        await example_basic_usage()
        await example_multiple_agents()
        await example_with_fallback()
        await example_cache_demonstration()
        await example_cost_monitoring()
        await example_health_check()
        await example_model_selection()
        
        print("\nüéâ Todos os exemplos executados com sucesso!")
        
    except Exception as e:
        print(f"\n‚ùå Erro durante execu√ß√£o: {e}")
        logger.exception("Erro nos exemplos")


if __name__ == "__main__":
    # Executar exemplos
    asyncio.run(main())